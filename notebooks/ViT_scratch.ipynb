{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from einops import rearrange, repeat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Patch Embedding\n",
    "It has three components:\n",
    "- Convert the image into sequence of patches\n",
    "- Add CLS token to sequence of patches\n",
    "- Add positional encoding to all the patches. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "class PatchEmbedding(nn.Module):\n",
    "    def __init__(self, config):\n",
    "        super().__init__()\n",
    "        image_height = config[\"image_height\"]\n",
    "        image_width = config[\"image_width\"]\n",
    "        im_channels = config[\"im_channels\"]\n",
    "        emb_dim = config[\"emb_dim\"] # Transformer dimentions(D)\n",
    "        patch_embd_dropout = config[\"patch_emb_dropout\"]\n",
    "\n",
    "        self.patch_height = config[\"patch_height\"]\n",
    "        self.patch_width = config[\"patch_width\"]\n",
    "\n",
    "        num_patches = (image_height // self.patch_height) * (image_width // self.patch_width)\n",
    "\n",
    "        patch_dim = im_channels * self.patch_height * self.patch_width    \n",
    "        \n",
    "        # W belongs to R^(patch_dim x emb_dim)\n",
    "        self.patch_emb = nn.Sequential(\n",
    "            nn.LayerNorm(patch_dim),           \n",
    "            nn.Linear(patch_dim, emb_dim),\n",
    "            nn.LayerNorm(emb_dim),\n",
    "        )\n",
    "        \n",
    "        # Positional information needs to be added to cls as well so 1+num_patches\n",
    "        self.pos_emb = nn.Parameter(torch.randn(1, num_patches + 1, emb_dim))\n",
    "        self.cls_token = nn.Parameter(torch.randn(emb_dim))    # CLS token belongs to R^emb_dim\n",
    "        self.patch_emb_dropout = nn.Dropout(patch_embd_dropout)\n",
    "\n",
    "    def forward(self, x):\n",
    "        batch_size = x.shape[0]\n",
    "\n",
    "        out  = rearrange(x, 'b c (h p1) (w p2) -> b (h w) (p1 p2 c)', p1=self.patch_height, p2=self.patch_width)  # split image into patches\n",
    "\n",
    "        out = self.patch_emb(out)\n",
    "        cls_token = repeat(self.cls_token, 'd -> b n d', b=batch_size, n=1)\n",
    "        out = torch.cat([cls_token, out], dim=1)\n",
    "        out += self.pos_emb\n",
    "        out = self.patch_emb_dropout(out)\n",
    "\n",
    "        return out\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 197, 512])\n"
     ]
    }
   ],
   "source": [
    "#example run\n",
    "image = torch.randn(1, 3, 224, 224)\n",
    "config = {\n",
    "    \"image_height\": 224,\n",
    "    \"image_width\": 224,\n",
    "    \"im_channels\": 3,\n",
    "    \"emb_dim\": 512,\n",
    "    \"patch_height\": 16,\n",
    "    \"patch_width\": 16,\n",
    "    \"patch_emb_dropout\": 0.1\n",
    "}\n",
    "patch_emb = PatchEmbedding(config)\n",
    "out = patch_emb(image)\n",
    "print(out.shape)  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Attention Module"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Attention(nn.Module):\n",
    "    def __init__(self, config):\n",
    "        super().__init__()\n",
    "        self.n_heads = config[\"n_heads\"]\n",
    "        self.head_dim = config[\"head_dim\"]   # d_h\n",
    "        self.emb_dim = config[\"emb_dim\"]\n",
    "        self.drop_prob = config[\"dropout\"] if \"dropout\" in config else 0.0\n",
    "        self.att_dim  = self.n_heads * self.head_dim\n",
    "\n",
    "        self.qkv_proj = nn.Linear(self.emb_dim, self.att_dim * 3, bias=False)\n",
    "        self.att_drop = nn.Dropout(self.drop_prob)\n",
    "\n",
    "        self.out_proj = nn.Sequential(\n",
    "            nn.Linear(self.att_dim, self.emb_dim),\n",
    "            nn.Dropout(self.drop_prob)\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        B, N = x.shape[:2]  # B: batch size, N: number of tokens\n",
    "\n",
    "        q, k,v = self.qkv_proj(x).split(self.att_dim, dim=-1)\n",
    "        #split into heads\n",
    "        q = rearrange(q, 'b n (h d_h) -> b h n d_h', h=self.n_heads, d_h=self.head_dim) \n",
    "        k = rearrange(k, 'b n (h d_h) -> b h n d_h', h=self.n_heads, d_h=self.head_dim)\n",
    "        v = rearrange(v, 'b n (h d_h) -> b h n d_h', h=self.n_heads, d_h=self.head_dim)\n",
    "\n",
    "        #Scaled dot product attention\n",
    "\n",
    "        att = torch.matmul(q, k.transpose(-2, -1)) / (self.head_dim ** 0.5)\n",
    "        att = nn.functional.softmax(att, dim=-1)\n",
    "        att = self.att_drop(att)\n",
    "\n",
    "\n",
    "        #Weighted Value Computation\n",
    "        out = torch.matmul(att, v)\n",
    "\n",
    "        #Rearrange heads\n",
    "        out = rearrange(out, 'b h n d_h -> b n (h d_h)', h=self.n_heads, d_h=self.head_dim)\n",
    "        out = self.out_proj(out)\n",
    "\n",
    "        return out\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 197, 512])\n"
     ]
    }
   ],
   "source": [
    "#example run\n",
    "config = {\n",
    "    \"n_heads\": 8,\n",
    "    \"head_dim\": 64,\n",
    "    \"emb_dim\": 512,\n",
    "    \"dropout\": 0.1\n",
    "}\n",
    "\n",
    "att = Attention(config)\n",
    "out = att(out)\n",
    "print(out.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Transformer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TransformerLayer(nn.Module):\n",
    "    def __init__(self, config):\n",
    "        super().__init__()\n",
    "        emb_dim = config[\"emb_dim\"]\n",
    "        ff_hidden_dim = config[\"ff_dim\"] if \"ff_dim\" in config else 4 * emb_dim\n",
    "        ff_dropout = config[\"ff_drop\"] if \"ff_drop\" in config else 0.0\n",
    "        self.att_norm = nn.LayerNorm(emb_dim)\n",
    "        self.ff_norm = nn.LayerNorm(emb_dim)\n",
    "        self.attention_block = Attention(config)\n",
    "        self.ff_block = nn.Sequential(\n",
    "            nn.Linear(emb_dim, ff_hidden_dim),\n",
    "            nn.GELU(),\n",
    "            nn.Dropout(ff_dropout),\n",
    "            nn.Linear(ff_hidden_dim, emb_dim),\n",
    "            nn.Dropout(ff_dropout)\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        out = x\n",
    "        out = out + self.attention_block(self.att_norm(out))\n",
    "        out = out + self.ff_block(self.ff_norm(out))\n",
    "        return out\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 197, 512])\n"
     ]
    }
   ],
   "source": [
    "#expample run\n",
    "config = {\n",
    "    \"emb_dim\": 512,\n",
    "    \"ff_dim\": 2048,\n",
    "    \"ff_drop\": 0.1,\n",
    "    \"n_heads\": 8,\n",
    "    \"head_dim\": 64,\n",
    "    \"dropout\": 0.1\n",
    "}\n",
    "\n",
    "trasnformer_block = TransformerLayer(config)\n",
    "out = trasnformer_block(out)\n",
    "print(out.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ViT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ViT(nn.Module):\n",
    "    def __init__(self, config):\n",
    "        super().__init__()\n",
    "        n_layers = config[\"n_layers\"]\n",
    "        emb_dim = config[\"emb_dim\"]\n",
    "        num_dim = config[\"num_dim\"] # number of classes\n",
    "        self.patch_embedding = PatchEmbedding(config)\n",
    "        self.transformer = nn.ModuleList([TransformerLayer(config) for _ in range(n_layers)])\n",
    "        self.norm = nn.LayerNorm(emb_dim)\n",
    "        self.fc_layer = nn.Linear(emb_dim, num_dim)\n",
    "\n",
    "    def forward(self, x):\n",
    "        out = self.patch_embedding(x)\n",
    "        for layer in self.transformer:\n",
    "            out = layer(out)\n",
    "        out = self.norm(out)\n",
    "        \n",
    "        out = self.fc_layer(out[:, 0])\n",
    "\n",
    "        #Logits, No softmax\n",
    "        return out\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 1000])\n"
     ]
    }
   ],
   "source": [
    "#example run\n",
    "image = torch.randn(1, 3, 224, 224)\n",
    "config = {\n",
    "    \"image_height\": 224,\n",
    "    \"image_width\": 224,\n",
    "    \"im_channels\": 3,\n",
    "    \"emb_dim\": 512,\n",
    "    \"patch_height\": 16,\n",
    "    \"patch_width\": 16,\n",
    "    \"patch_emb_dropout\": 0.1,\n",
    "    \"n_layers\": 12,\n",
    "    \"num_dim\": 1000,\n",
    "    \"n_heads\": 8,\n",
    "    \"head_dim\": 64,\n",
    "    \"dropout\": 0.1\n",
    "}\n",
    "\n",
    "model = ViT(config)\n",
    "out = model(image)\n",
    "print(out.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "fedl",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
